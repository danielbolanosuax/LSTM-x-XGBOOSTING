## ===============================
##         IMPORTAR LIBRER√çAS
## ===============================
import numpy as np
import pandas as pd
import datetime
import time
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from alpha_vantage.timeseries import TimeSeries
from alpha_vantage.fundamentaldata import FundamentalData
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import xgboost as xgb
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import plotly.graph_objects as go
import yfinance as yf
import warnings
warnings.filterwarnings('ignore')  # Para suprimir algunas advertencias de librer√≠as


## ===============================
##  API KEY DE ALPHA VANTAGE Y FUNCI√ìN PARA DESCARGAR DATOS
## ===============================
ALPHA_VANTAGE_API_KEY = "6XE23J2QP58EE8L7"

def obtener_datos_alpha_vantage(ticker, api_key, intentos=5):
    ts = TimeSeries(key=api_key, output_format='pandas')
    for i in range(intentos):
        try:
            print(f"Descargando datos para {ticker} (Intento {i+1}/{intentos})...")
            data, _ = ts.get_daily(symbol=ticker, outputsize='full')
            data.columns = ['Open', 'High', 'Low', 'Close', 'Volume']
            data.index = pd.to_datetime(data.index)
            return data.sort_index()
        except Exception as e:
            print(f"Error: {e}")
            time.sleep(5)
    raise Exception("No se pudieron descargar los datos.")


## ===============================
##        DESCARGA DE DATOS
## ===============================
ticker = 'NVDA'
start_date = datetime.datetime(2022, 1, 1)

data = obtener_datos_alpha_vantage(ticker, ALPHA_VANTAGE_API_KEY)
data = data.loc[data.index >= start_date]
print(f"√öltima fecha disponible en los datos: {data.index.max()}")


## ===============================
##   C√ÅLCULO DE INDICADORES T√âCNICOS
## ===============================

# --- C√°lculo CORRECTO del RSI con Media M√≥vil Exponencial ---
window_rsi = 14
delta = data['Close'].diff(1)
gain = np.where(delta > 0, delta, 0)
loss = np.where(delta < 0, -delta, 0)
avg_gain = pd.Series(gain, index=data.index).ewm(span=window_rsi, adjust=False).mean()
avg_loss = pd.Series(loss, index=data.index).ewm(span=window_rsi, adjust=False).mean()
rs = avg_gain / (avg_loss + 1e-10)  # Evitar divisi√≥n por cero
data['RSI'] = 100 - (100 / (1 + rs))

# --- C√°lculo del MACD, L√≠nea de Se√±al y Histograma ---
data['MACD'] = data['Close'].ewm(span=12, adjust=False).mean() - data['Close'].ewm(span=26, adjust=False).mean()
data['Signal_Line'] = data['MACD'].ewm(span=9, adjust=False).mean()
data['MACD_Histogram'] = data['MACD'] - data['Signal_Line']

# --- C√°lculo del Estoc√°stico ---
data['%K'] = 100 * (data['Close'] - data['Low'].rolling(14).min()) / (data['High'].rolling(14).max() - data['Low'].rolling(14).min())
data['%D'] = data['%K'].rolling(3).mean()

# --- √çndice de Capital Institucional (ICI) basado en Volumen ---
data['Volume_MA_50'] = data['Volume'].rolling(window=50).mean()
data['Institutional_Index'] = data['Volume'] / data['Volume_MA_50']

# --- C√°lculo del OBV (On Balance Volume) ---
data['OBV'] = (np.sign(data['Close'].diff()) * data['Volume']).fillna(0).cumsum()

# --- Definir Entrada y Salida de Capital basados en umbrales ---
entrada_umbral = 2.0
salida_umbral = 0.5
data['Entrada_Capital'] = (data['Institutional_Index'] > entrada_umbral).astype(int)
data['Salida_Capital'] = (data['Institutional_Index'] < salida_umbral).astype(int)


## ===============================
##   PREPROCESAMIENTO DE DATOS
## ===============================
data.fillna(method='ffill', inplace=True)
data.dropna(inplace=True)

# --- Seleccionar caracter√≠sticas para el modelo ---
features = ['RSI', 'MACD', 'Signal_Line', '%K', '%D', 'Volume', 'Institutional_Index', 'OBV']
X = data[features].values
y = data['Entrada_Capital'].values  # Etiqueta binaria (Entrada de capital)

# --- Normalizaci√≥n de Datos ---
scaler = MinMaxScaler(feature_range=(0, 1))
X_scaled = scaler.fit_transform(X)


## ===============================
##  FUNCI√ìN PARA CREAR SECUENCIAS (VENTANAS) PARA LSTM
## ===============================
def create_sequences(X_data, y_data, window_size=30):
    """
    Crea secuencias de longitud 'window_size' a partir de X_data y la etiqueta correspondiente en y_data.
    Retorna:
      - X_seq: array de forma (num_muestras, window_size, num_features)
      - y_seq: array de forma (num_muestras,)
    """
    Xs, ys = [], []
    for i in range(len(X_data) - window_size):
        Xs.append(X_data[i:(i+window_size), :])
        ys.append(y_data[i + window_size])  # la etiqueta correspondiente al final de la ventana
    return np.array(Xs), np.array(ys)


## ===============================
##   DEFINICI√ìN DE MODELO LSTM
## ===============================
def build_lstm_model(neurons1, neurons2, dropout, learning_rate, window_size, num_features):
    """
    Construye un modelo LSTM con 2 capas LSTM y capas Dense adicionales.
    """
    model = Sequential([
        LSTM(neurons1, return_sequences=True, input_shape=(window_size, num_features)),
        Dropout(dropout),
        LSTM(neurons2),
        Dropout(dropout),
        Dense(64, activation='relu'),
        Dropout(dropout),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model


## ===============================
##   FUNCI√ìN PARA OPTIMIZACI√ìN MANUAL LSTM
## ===============================
def optimize_lstm(X_train, y_train, X_test, y_test, param_grid, window_size):
    best_acc = 0
    best_model = None
    best_params = {}
    
    # EarlyStopping para detener el entrenamiento si no hay mejora
    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    
    num_features = X_train.shape[2]  # Dimensi√≥n de features en cada timestep

    for neurons1 in param_grid['neurons1']:
        for neurons2 in param_grid['neurons2']:
            for dropout in param_grid['dropout']:
                for learning_rate in param_grid['learning_rate']:
                    for batch_size in param_grid['batch_size']:
                        for epochs in param_grid['epochs']:
                            model = build_lstm_model(neurons1, neurons2, dropout,
                                                     learning_rate, window_size,
                                                     num_features)
                            model.fit(X_train, y_train,
                                      epochs=epochs,
                                      batch_size=batch_size,
                                      validation_data=(X_test, y_test),
                                      callbacks=[early_stop],
                                      verbose=0)
                            acc = model.evaluate(X_test, y_test, verbose=0)[1]
                            
                            if acc > best_acc:
                                best_acc = acc
                                best_model = model
                                best_params = {
                                    'neurons1': neurons1,
                                    'neurons2': neurons2,
                                    'dropout': dropout,
                                    'learning_rate': learning_rate,
                                    'batch_size': batch_size,
                                    'epochs': epochs
                                }
    return best_model, best_params


## ===============================
##  VALIDACI√ìN CRUZADA CON TimeSeriesSplit
## ===============================
tscv = TimeSeriesSplit(n_splits=5)
fold = 1
metrics_list = []
best_params_folds = []
y_test_final = None
y_pred_final = None

# Par√°metros de la LSTM a buscar
param_grid_lstm = {
    'neurons1': [64, 128],
    'neurons2': [25, 50],
    'dropout': [0.2, 0.3],
    'learning_rate': [0.001, 0.0005],
    'batch_size': [16, 32],
    'epochs': [30, 50]
}

# Par√°metros de XGBoost a buscar
param_grid_xgb = {
    'n_estimators': [100, 200],
    'learning_rate': [0.1, 0.05],
    'max_depth': [6, 8],
    'scale_pos_weight': [10, 15]
}

window_size = 30  # Tama√±o de ventana para la LSTM

for train_index, test_index in tscv.split(X_scaled):
    print(f"\n--- Fold {fold} ---")
    
    # Dividir datos en train y test
    X_train_fold = X_scaled[train_index]
    y_train_fold = y[train_index]
    X_test_fold = X_scaled[test_index]
    y_test_fold = y[test_index]
    
    # Aplicar SMOTE en el conjunto de entrenamiento (OJO: no es lo ideal en series temporales)
    minority_class_samples = np.sum(y_train_fold == 1)
    if minority_class_samples >= 2:
        print(f"Aplicando SMOTE en Fold {fold} con {minority_class_samples} ejemplos de la clase minoritaria...")
        smote = SMOTE(random_state=42, k_neighbors=min(3, minority_class_samples-1))
        X_train_fold, y_train_fold = smote.fit_resample(X_train_fold, y_train_fold)
    
    # Crear secuencias para LSTM
    X_train_seq, y_train_seq = create_sequences(X_train_fold, y_train_fold, window_size=window_size)
    X_test_seq, y_test_seq = create_sequences(X_test_fold, y_test_fold, window_size=window_size)
    
    # Si el test_seq queda muy peque√±o o vac√≠o, se salta (solo por seguridad)
    if len(X_test_seq) == 0:
        print(f"Fold {fold}: no hay suficientes muestras en el conjunto de test despu√©s de crear secuencias.")
        fold += 1
        continue
    
    # Optimizaci√≥n manual de hiperpar√°metros para LSTM
    best_lstm_model, best_params = optimize_lstm(
        X_train_seq, y_train_seq,
        X_test_seq, y_test_seq,
        param_grid_lstm, window_size
    )
    print(f"Fold {fold} - Mejores par√°metros LSTM: {best_params}")
    
    # Generar features con la salida de la LSTM
    X_train_lstm_features = best_lstm_model.predict(X_train_seq).flatten()
    X_test_lstm_features = best_lstm_model.predict(X_test_seq).flatten()
    
    # Ajustar XGBoost con RandomizedSearchCV (usando las caracter√≠sticas generadas)
    grid_xgb = RandomizedSearchCV(
        estimator=xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
        param_distributions=param_grid_xgb,
        cv=TimeSeriesSplit(n_splits=3),
        scoring='accuracy',
        n_iter=5,
        random_state=42
    )
    grid_xgb.fit(X_train_lstm_features.reshape(-1, 1), y_train_seq)
    print(f"Fold {fold} - Mejores par√°metros XGBoost: {grid_xgb.best_params_}")
    
    # Entrenar el mejor modelo XGBoost y predecir en test
    best_xgb_model = grid_xgb.best_estimator_
    y_pred = best_xgb_model.predict(X_test_lstm_features.reshape(-1, 1))
    
    # Evaluaci√≥n
    acc = accuracy_score(y_test_seq, y_pred)
    prec = precision_score(y_test_seq, y_pred, zero_division=0)
    rec = recall_score(y_test_seq, y_pred, zero_division=0)
    f1 = f1_score(y_test_seq, y_pred, zero_division=0)
    auc = roc_auc_score(y_test_seq, y_pred)
    
    print(f"Fold {fold} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}, AUC-ROC: {auc:.4f}")
    
    metrics_list.append({
        'fold': fold,
        'accuracy': acc,
        'precision': prec,
        'recall': rec,
        'f1_score': f1,
        'auc_roc': auc
    })
    
    best_params_folds.append({
        'fold': fold,
        'lstm_params': best_params,
        'xgb_params': grid_xgb.best_params_
    })
    
    # Guardar resultados del √∫ltimo fold para visualizaci√≥n
    y_test_final = y_test_seq
    y_pred_final = y_pred
    
    fold += 1

# Mostrar m√©tricas promedio
if metrics_list:
    avg_metrics = {key: np.mean([m[key] for m in metrics_list]) for key in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']}
    print("\n--- M√©tricas Promedio en Cross-Validation ---")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.4f}")

    # Mostrar los mejores par√°metros obtenidos en cada fold
    print("\n--- Mejores Par√°metros por Fold ---")
    for params in best_params_folds:
        print(params)

    # Visualizaci√≥n: Matriz de Confusi√≥n para el √∫ltimo fold
    plt.figure(figsize=(6, 5))
    sns.heatmap(confusion_matrix(y_test_final, y_pred_final), annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicho')
    plt.ylabel('Real')
    plt.title('Matriz de Confusi√≥n - XGBoost (√öltimo Fold)')
    plt.show()


## ===============================
##       VISUALIZACI√ìN RSI
## ===============================
plt.figure(figsize=(12, 5))
plt.plot(data.index, data['RSI'], color='purple', label="RSI (14)")
plt.axhline(70, linestyle='--', color='red', alpha=0.5, label="Sobrecompra (70)")
plt.axhline(30, linestyle='--', color='green', alpha=0.5, label="Sobreventa (30)")
plt.ylim(0, 100)
plt.title(f'√çndice de Fuerza Relativa (RSI) de {ticker}')
plt.legend()
plt.show()


## ===============================
##       VISUALIZACI√ìN MACD
## ===============================
plt.figure(figsize=(12, 5))
plt.bar(data.index, data['MACD_Histogram'], color='gray', label="MACD Histogram")
plt.plot(data.index, data['MACD'], color='blue', label="MACD Line")
plt.plot(data.index, data['Signal_Line'], color='orange', label="L√≠nea de Se√±al")
plt.title(f'MACD de {ticker}')
plt.legend()
plt.show()


## ===============================
##       VISUALIZACI√ìN OBV
## ===============================
plt.figure(figsize=(12, 5))
plt.plot(data.index, data['OBV'], color='blue', label="OBV")
plt.xlabel("Fecha")
plt.ylabel("OBV")
plt.title(f'On Balance Volume (OBV) de {ticker}')
plt.legend()
plt.show()


## ===============================
##   DATOS FUNDAMENTALES DESDE ALPHA VANTAGE
## ===============================
if 'Valoraci√≥n' not in data.columns:
    print("‚ö†Ô∏è Advertencia: La columna 'Valoraci√≥n' no existe en el DataFrame. Se asignar√° un valor por defecto.")
    data['Valoraci√≥n'] = np.nan

def obtener_valuacion_alpha(ticker, api_key):
    try:
        fd = FundamentalData(key=api_key, output_format='pandas')
        data_fundamental, _ = fd.get_company_overview(symbol=ticker)
        market_cap = float(data_fundamental.loc['MarketCapitalization'].values[0])
        return market_cap
    except Exception as e:
        print(f"‚ùå Error obteniendo datos fundamentales de Alpha Vantage: {e}")
        return np.nan

valuation_index = obtener_valuacion_alpha(ticker, ALPHA_VANTAGE_API_KEY)

if not np.isnan(valuation_index):
    data['Valoraci√≥n'] = np.full(len(data), valuation_index, dtype=np.float64)
else:
    data['Valoraci√≥n'] = 2.5  # Valor por defecto


## ===============================
##   NUEVA CONDICI√ìN DE COMPRA
## ===============================
buy_signal = (data['RSI'] < 30) & \
             (data['%K'] < 20) & \
             (data['MACD'] < -5) & \
             (data['Valoraci√≥n'] < 2.5) & \
             (data['OBV'].diff() > 0)
data['Se√±al_Compra'] = 0
data.loc[buy_signal, 'Se√±al_Compra'] = 1


## ===============================
##  GR√ÅFICO DE VELAS JAPONESAS (PLOTLY)
## ===============================
fig = go.Figure()
fig.add_trace(go.Candlestick(
    x=data.index, 
    open=data['Open'], 
    high=data['High'], 
    low=data['Low'], 
    close=data['Close'], 
    name='Precio'
))
fig.add_trace(go.Scatter(
    x=data.index[data['Entrada_Capital'] == 1], 
    y=data['Close'][data['Entrada_Capital'] == 1], 
    mode='markers', 
    marker=dict(size=10, color='green', symbol='triangle-up'), 
    name='Entrada de Capital'
))
fig.add_trace(go.Scatter(
    x=data.index[data['Salida_Capital'] == 1], 
    y=data['Close'][data['Salida_Capital'] == 1], 
    mode='markers', 
    marker=dict(size=10, color='blue', symbol='triangle-down'), 
    name='Salida de Capital'
))
# Agregar se√±ales de compra
fig.add_trace(go.Scatter(
    x=data.index[data['Se√±al_Compra'] == 1], 
    y=data['Close'][data['Se√±al_Compra'] == 1], 
    mode='markers', 
    marker=dict(size=10, color='yellow', symbol='triangle-up'), 
    name='Se√±al de Compra'
))
fig.update_layout(
    title=f'Se√±ales Institucionales para {ticker}',
    xaxis_title='Fecha',
    yaxis_title='Precio',
    xaxis_rangeslider_visible=False,
    template='plotly_dark',
    height=600,
    width=1000
)
fig.show()


## ===============================
##  VERIFICAR VALORES DE 'VALORACI√ìN'
## ===============================
print("Valores de 'Valoraci√≥n':")
print(data[['Valoraci√≥n']].dropna().head())


## ===============================
##  OBTENCI√ìN DE DATOS FUNDAMENTALES DESDE YAHOO FINANCE
## ===============================
def obtener_valuacion_yahoo(ticker):
    try:
        print(f"üìä Descargando datos fundamentales para {ticker} desde Yahoo Finance...")
        stock = yf.Ticker(ticker)
        info = stock.info
        pe_ratio = info.get("trailingPE", None)
        pb_ratio = info.get("priceToBook", None)
        ev_to_ebitda = info.get("enterpriseToEbitda", None)
        valid_metrics = [m for m in [pe_ratio, pb_ratio, ev_to_ebitda] if m is not None]
        valuation_index = sum(valid_metrics) / len(valid_metrics) if valid_metrics else None
        return pe_ratio, pb_ratio, ev_to_ebitda, valuation_index
    except Exception as e:
        print(f"‚ùå Error obteniendo datos fundamentales: {e}")
        return None, None, None, None

pe_ratio, pb_ratio, ev_to_ebitda, valuation_index_yahoo = obtener_valuacion_yahoo(ticker)

if valuation_index_yahoo is not None:
    print(f"üìä P/E Ratio: {pe_ratio if pe_ratio else 'N/A'} | P/B Ratio: {pb_ratio if pb_ratio else 'N/A'} | EV/EBITDA: {ev_to_ebitda if ev_to_ebitda else 'N/A'}")
    print(f"üìä √çndice de Valoraci√≥n (Yahoo): {valuation_index_yahoo:.2f}")
    if valuation_index_yahoo > 2.5:
        print("‚ö†Ô∏è La empresa est√° MUY SOBREVALORADA üìà (Riesgo de ca√≠da)")
    elif valuation_index_yahoo < 1:
        print("‚úÖ La empresa est√° INFRAVALORADA üìâ (Oportunidad de compra)")
    else:
        print("üîç La empresa tiene una valoraci√≥n razonable.")
else:
    print("‚ùå No se pudo calcular el √≠ndice de valoraci√≥n desde Yahoo Finance.")


## ===============================
##   AGREGAR 'VALORACI√ìN' COMO FEATURE (OPCIONAL)
## ===============================
features = ['RSI', 'MACD', 'Signal_Line', '%K', '%D', 'Volume', 'Institutional_Index', 'Valoraci√≥n']
X = data[features].values
# Si es necesario, se puede normalizar X nuevamente o ajustar el pipeline.


## ===============================
##  GR√ÅFICO DEL √çNDICE DE VALORACI√ìN
## ===============================
plt.figure(figsize=(12, 5))
data['Valoraci√≥n_Suave'] = data['Valoraci√≥n'].rolling(window=5).mean()
plt.plot(data.index, data['Valoraci√≥n_Suave'], color='blue', linewidth=2, label="√çndice de Valoraci√≥n (Suavizado)")
min_valor = data['Valoraci√≥n'].min()
max_valor = data['Valoraci√≥n'].max()
margen = (max_valor - min_valor) * 0.1
plt.ylim(min_valor - margen, max_valor + margen)
plt.axhline(2.5, linestyle='--', color='red', alpha=0.5, label="Sobrevalorada (>2.5)")
plt.axhline(1, linestyle='--', color='green', alpha=0.5, label="Infravalorada (<1)")
plt.title(f'√çndice de Valoraci√≥n de {ticker}')
plt.legend()
plt.show()
