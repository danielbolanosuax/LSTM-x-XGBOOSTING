{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Configuraci√≥n Inicial y Reproducibilidad\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Importar Librer√≠as y Funciones de Descarga y Preprocesamiento\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "from alpha_vantage.fundamentaldata import FundamentalData\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import plotly.graph_objects as go\n",
    "import yfinance as yf\n",
    "\n",
    "# --- API Key y funci√≥n para descargar datos ---\n",
    "ALPHA_VANTAGE_API_KEY = \"6XE23J2QP58EE8L7\"\n",
    "\n",
    "def obtener_datos_alpha_vantage(ticker, api_key, intentos=5):\n",
    "    ts = TimeSeries(key=api_key, output_format='pandas')\n",
    "    for i in range(intentos):\n",
    "        try:\n",
    "            print(f\"Descargando datos para {ticker} (Intento {i+1}/{intentos})...\")\n",
    "            data, _ = ts.get_daily(symbol=ticker, outputsize='full')\n",
    "            data.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "            return data.sort_index()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            time.sleep(5)\n",
    "    raise Exception(\"No se pudieron descargar los datos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando datos para TTD (Intento 1/5)...\n",
      "√öltima fecha disponible en los datos: 2025-03-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "## üìå Configuraci√≥n del Ticker, Descarga de Datos e Indicadores T√©cnicos\n",
    "ticker = 'TTD'\n",
    "start_date = datetime.datetime(2022, 1, 1)\n",
    "data = obtener_datos_alpha_vantage(ticker, ALPHA_VANTAGE_API_KEY)\n",
    "data = data.loc[data.index >= start_date]\n",
    "print(f\"√öltima fecha disponible en los datos: {data.index.max()}\")\n",
    "\n",
    "# --- C√°lculo del RSI ---\n",
    "window_rsi = 14\n",
    "delta = data['Close'].diff(1)\n",
    "gain = np.where(delta > 0, delta, 0)\n",
    "loss = np.where(delta < 0, -delta, 0)\n",
    "avg_gain = pd.Series(gain, index=data.index).ewm(span=window_rsi, adjust=False).mean()\n",
    "avg_loss = pd.Series(loss, index=data.index).ewm(span=window_rsi, adjust=False).mean()\n",
    "rs = avg_gain / (avg_loss + 1e-10)\n",
    "data['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# --- MACD, L√≠nea de Se√±al y Histograma ---\n",
    "data['MACD'] = data['Close'].ewm(span=12, adjust=False).mean() - data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['Signal_Line'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "data['MACD_Histogram'] = data['MACD'] - data['Signal_Line']\n",
    "\n",
    "# --- Estoc√°stico ---\n",
    "data['%K'] = 100 * (data['Close'] - data['Low'].rolling(14).min()) / (data['High'].rolling(14).max() - data['Low'].rolling(14).min())\n",
    "data['%D'] = data['%K'].rolling(3).mean()\n",
    "\n",
    "# --- √çndice de Capital Institucional (ICI) basado en Volumen ---\n",
    "data['Volume_MA_50'] = data['Volume'].rolling(window=50).mean()\n",
    "data['Institutional_Index'] = data['Volume'] / data['Volume_MA_50']\n",
    "\n",
    "# --- OBV ---\n",
    "data['OBV'] = (np.sign(data['Close'].diff()) * data['Volume']).fillna(0).cumsum()\n",
    "\n",
    "# --- Definir Entrada y Salida de Capital ---\n",
    "entrada_umbral = 2.0\n",
    "salida_umbral = 0.5\n",
    "data['Entrada_Capital'] = (data['Institutional_Index'] > entrada_umbral).astype(int)\n",
    "data['Salida_Capital'] = (data['Institutional_Index'] < salida_umbral).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Preprocesamiento de Datos y Normalizaci√≥n\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# --- Seleccionar caracter√≠sticas ---\n",
    "features = ['RSI', 'MACD', 'Signal_Line', '%K', '%D', 'Volume', 'Institutional_Index', 'OBV']\n",
    "X = data[features].values\n",
    "y = data['Entrada_Capital'].values  \n",
    "\n",
    "# --- Normalizaci√≥n ---\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Definir input_dim para el modelo LSTM\n",
    "input_dim = X_scaled.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## Wrapper Personalizado para KerasClassifier (Soluci√≥n para Python 3.12)\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from types import SimpleNamespace\n",
    "\n",
    "class MyKerasClassifier(KerasClassifier):\n",
    "    def _more_tags(self):\n",
    "        return {\n",
    "            \"estimator_type\": \"classifier\",\n",
    "            \"requires_y\": True,\n",
    "            \"classifier_tags\": {\"multi_class\": False, \"multi_label\": False}\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "## üìå Pre-compilaci√≥n de predict para reducir retracing\n",
    "dummy_input = np.reshape(X_scaled[:1], (1, 1, X_scaled.shape[1]))\n",
    "_ = build_lstm_model(64, 25, 0.2, 0.001).predict(dummy_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## Funci√≥n para Optimizar el Modelo LSTM con RandomizedSearchCV (parche para __sklearn_tags__)\n",
    "def optimize_lstm(X_train, y_train, X_val, y_val, param_dist, cv_splits=3):\n",
    "    # Configuramos EarlyStopping para evitar sobreentrenar\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Creamos la instancia del wrapper personalizado\n",
    "    lstm_classifier = MyKerasClassifier(model=build_lstm_model, fit__callbacks=[early_stop], verbose=0)\n",
    "    \n",
    "    # Forzamos que la instancia devuelva las tags esperadas usando SimpleNamespace\n",
    "    from types import SimpleNamespace\n",
    "    lstm_classifier.__sklearn_tags__ = lambda: SimpleNamespace(\n",
    "        estimator_type=\"classifier\",\n",
    "        requires_y=True,\n",
    "        classifier_tags=SimpleNamespace(multi_class=False, multi_label=False)\n",
    "    )\n",
    "    \n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    random_search = RandomizedSearchCV(estimator=lstm_classifier,\n",
    "                                       param_distributions=param_dist,\n",
    "                                       cv=TimeSeriesSplit(n_splits=cv_splits),\n",
    "                                       scoring='accuracy',\n",
    "                                       n_iter=5,\n",
    "                                       random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    return random_search.best_estimator_.model, random_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Configuraci√≥n de Variables para la Validaci√≥n Cruzada\n",
    "fold = 1\n",
    "metrics_list = []\n",
    "best_params_folds = []\n",
    "y_test_final = None\n",
    "y_pred_final = None\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1 ---\n",
      "Aplicando SMOTE en Fold 1 con 7 muestras de la clase minoritaria...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'classifier_tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[144]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# --- Optimizaci√≥n de hiperpar√°metros para LSTM ---\u001b[39;00m\n\u001b[32m     22\u001b[39m param_dist_lstm = {\n\u001b[32m     23\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mneurons1\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m64\u001b[39m, \u001b[32m128\u001b[39m],\n\u001b[32m     24\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mneurons2\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m25\u001b[39m, \u001b[32m50\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m30\u001b[39m, \u001b[32m50\u001b[39m]\n\u001b[32m     29\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m best_lstm_model, best_params = \u001b[43moptimize_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_dist_lstm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Mejores par√°metros LSTM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# --- Generar caracter√≠sticas usando el mejor modelo LSTM ---\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36moptimize_lstm\u001b[39m\u001b[34m(X_train, y_train, X_val, y_val, param_dist, cv_splits)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomizedSearchCV\n\u001b[32m     19\u001b[39m random_search = RandomizedSearchCV(estimator=lstm_classifier,\n\u001b[32m     20\u001b[39m                                    param_distributions=param_dist,\n\u001b[32m     21\u001b[39m                                    cv=TimeSeriesSplit(n_splits=cv_splits),\n\u001b[32m     22\u001b[39m                                    scoring=\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     23\u001b[39m                                    n_iter=\u001b[32m5\u001b[39m,\n\u001b[32m     24\u001b[39m                                    random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mrandom_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m random_search.best_estimator_.model, random_search.best_params_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:933\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    929\u001b[39m params = _check_method_params(X, params=params)\n\u001b[32m    931\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._get_routed_params_for_fit(params)\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m cv_orig = check_cv(\u001b[38;5;28mself\u001b[39m.cv, y, classifier=\u001b[43mis_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    934\u001b[39m n_splits = cv_orig.get_n_splits(X, y, **routed_params.splitter.split)\n\u001b[32m    936\u001b[39m base_estimator = clone(\u001b[38;5;28mself\u001b[39m.estimator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1237\u001b[39m, in \u001b[36mis_classifier\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m   1230\u001b[39m     warnings.warn(\n\u001b[32m   1231\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpassing a class to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mprint\u001b[39m(inspect.stack()[\u001b[32m0\u001b[39m][\u001b[32m3\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1232\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in 1.8. Use an instance of the class instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1233\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m   1234\u001b[39m     )\n\u001b[32m   1235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m_estimator_type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m.estimator_type == \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_tags.py:443\u001b[39m, in \u001b[36mget_tags\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m    441\u001b[39m current_name, current_tags = current_item\n\u001b[32m    442\u001b[39m next_name, next_tags = next_item\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m current_tags = \u001b[43m_to_old_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_tags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m next_tags = _to_old_tags(next_tags)\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# Compare tags and store differences\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_tags.py:552\u001b[39m, in \u001b[36m_to_old_tags\u001b[39m\u001b[34m(new_tags)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_to_old_tags\u001b[39m(new_tags):\n\u001b[32m    551\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Utility function convert old tags (dictionary) to new tags (dataclass).\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnew_tags\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclassifier_tags\u001b[49m:\n\u001b[32m    553\u001b[39m         binary_only = \u001b[38;5;129;01mnot\u001b[39;00m new_tags.classifier_tags.multi_class\n\u001b[32m    554\u001b[39m         multilabel = new_tags.classifier_tags.multi_label\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'classifier_tags'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "## üìå Bucle de Validaci√≥n Cruzada y Optimizaci√≥n\n",
    "\n",
    "for train_index, test_index in tscv.split(X_scaled):\n",
    "    print(f\"\\n--- Fold {fold} ---\")\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # --- Aplicar SMOTE si hay al menos 2 muestras de la clase minoritaria ---\n",
    "    minority_class_samples = np.sum(y_train == 1)\n",
    "    if minority_class_samples >= 2:\n",
    "        print(f\"Aplicando SMOTE en Fold {fold} con {minority_class_samples} muestras de la clase minoritaria...\")\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        smote = SMOTE(random_state=42, k_neighbors=min(3, minority_class_samples-1))\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # --- Remodelar datos para LSTM ---\n",
    "    X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    # --- Optimizaci√≥n de hiperpar√°metros para LSTM ---\n",
    "    param_dist_lstm = {\n",
    "        'neurons1': [64, 128],\n",
    "        'neurons2': [25, 50],\n",
    "        'dropout': [0.2, 0.3],\n",
    "        'learning_rate': [0.001, 0.0005],\n",
    "        'batch_size': [16, 32],\n",
    "        'epochs': [30, 50]\n",
    "    }\n",
    "    best_lstm_model, best_params = optimize_lstm(X_train_reshaped, y_train, X_test_reshaped, y_test, param_dist_lstm)\n",
    "    print(f\"Fold {fold} - Mejores par√°metros LSTM: {best_params}\")\n",
    "    \n",
    "    # --- Generar caracter√≠sticas usando el mejor modelo LSTM ---\n",
    "    X_train_lstm_features = best_lstm_model.predict(X_train_reshaped).flatten()\n",
    "    X_test_lstm_features = best_lstm_model.predict(X_test_reshaped).flatten()\n",
    "    \n",
    "    # --- Optimizaci√≥n de hiperpar√°metros para XGBoost ---\n",
    "    param_dist_xgb = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.1, 0.05],\n",
    "        'max_depth': [6, 8],\n",
    "        'scale_pos_weight': [10, 15]\n",
    "    }\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    grid_xgb = RandomizedSearchCV(estimator=xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "                                  param_distributions=param_dist_xgb, \n",
    "                                  cv=TimeSeriesSplit(n_splits=3), \n",
    "                                  scoring='accuracy', \n",
    "                                  n_iter=5, \n",
    "                                  random_state=42)\n",
    "    grid_xgb.fit(X_train_lstm_features.reshape(-1, 1), y_train)\n",
    "    print(f\"Fold {fold} - Mejores par√°metros XGBoost: {grid_xgb.best_params_}\")\n",
    "    \n",
    "    best_params_folds.append({\n",
    "        'fold': fold,\n",
    "        'lstm_params': best_params,\n",
    "        'xgb_params': grid_xgb.best_params_\n",
    "    })\n",
    "    \n",
    "    best_xgb_model = grid_xgb.best_estimator_\n",
    "    y_pred = best_xgb_model.predict(X_test_lstm_features.reshape(-1, 1))\n",
    "    \n",
    "    # --- Evaluaci√≥n del fold ---\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    print(f\"Fold {fold} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}, AUC-ROC: {auc:.4f}\")\n",
    "    \n",
    "    metrics_list.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1_score': f1,\n",
    "        'auc_roc': auc\n",
    "    })\n",
    "    \n",
    "    y_test_final = y_test\n",
    "    y_pred_final = y_pred\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "avg_metrics = {key: np.mean([m[key] for m in metrics_list]) for key in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']}\n",
    "print(\"\\n--- M√©tricas Promedio en Cross-Validation ---\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n--- Mejores Par√°metros por Fold ---\")\n",
    "for params in best_params_folds:\n",
    "    print(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Visualizaci√≥n: Matriz de Confusi√≥n para el √∫ltimo fold\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_matrix(y_test_final, y_pred_final), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicho')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusi√≥n - XGBoost (√öltimo Fold)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Visualizaci√≥n: Gr√°fico del RSI\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(data.index, data['RSI'], color='purple', label=\"RSI (14)\")\n",
    "plt.axhline(70, linestyle='--', color='red', alpha=0.5, label=\"Sobrecompra (70)\")\n",
    "plt.axhline(30, linestyle='--', color='green', alpha=0.5, label=\"Sobreventa (30)\")\n",
    "plt.ylim(0, 100)\n",
    "plt.title(f'√çndice de Fuerza Relativa (RSI) de {ticker}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Visualizaci√≥n: Gr√°fico MACD\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(data.index, data['MACD_Histogram'], color='gray', label=\"MACD Histogram\")\n",
    "plt.plot(data.index, data['MACD'], color='blue', label=\"MACD Line\")\n",
    "plt.plot(data.index, data['Signal_Line'], color='orange', label=\"L√≠nea de Se√±al\")\n",
    "plt.title(f'MACD de {ticker}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Visualizaci√≥n: Gr√°fico del OBV\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(data.index, data['OBV'], color='blue', label=\"OBV\")\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"OBV\")\n",
    "plt.title(f'On Balance Volume (OBV) de {ticker}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Datos Fundamentales desde Alpha Vantage y asignaci√≥n de 'Valoraci√≥n'\n",
    "if 'Valoraci√≥n' not in data.columns:\n",
    "    print(\"‚ö†Ô∏è Advertencia: La columna 'Valoraci√≥n' no existe en el DataFrame. Se asignar√° un valor por defecto.\")\n",
    "    data['Valoraci√≥n'] = np.nan\n",
    "\n",
    "def obtener_valuacion_alpha(ticker, api_key):\n",
    "    try:\n",
    "        fd = FundamentalData(key=api_key, output_format='pandas')\n",
    "        data_fundamental, _ = fd.get_company_overview(symbol=ticker)\n",
    "        market_cap = float(data_fundamental.loc['MarketCapitalization'].values[0])\n",
    "        return market_cap\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error obteniendo datos fundamentales de Alpha Vantage: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "valuation_index = obtener_valuacion_alpha(ticker, ALPHA_VANTAGE_API_KEY)\n",
    "if not np.isnan(valuation_index):\n",
    "    data['Valoraci√≥n'] = np.full(len(data), valuation_index, dtype=np.float64)\n",
    "else:\n",
    "    data['Valoraci√≥n'] = 2.5  # Valor por defecto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Nueva condici√≥n de compra con OBV y otros indicadores\n",
    "buy_signal = (data['RSI'] < 30) & \\\n",
    "             (data['%K'] < 20) & \\\n",
    "             (data['MACD'] < -5) & \\\n",
    "             (data['Valoraci√≥n'] < 2.5) & \\\n",
    "             (data['OBV'].diff() > 0)\n",
    "data['Se√±al_Compra'] = 0\n",
    "data.loc[buy_signal, 'Se√±al_Compra'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Gr√°fico de Velas Japonesas con Se√±ales Institucionales usando Plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Candlestick(\n",
    "    x=data.index, \n",
    "    open=data['Open'], \n",
    "    high=data['High'], \n",
    "    low=data['Low'], \n",
    "    close=data['Close'], \n",
    "    name='Precio'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.index[data['Entrada_Capital'] == 1], \n",
    "    y=data['Close'][data['Entrada_Capital'] == 1], \n",
    "    mode='markers', \n",
    "    marker=dict(size=10, color='green', symbol='triangle-up'), \n",
    "    name='Entrada de Capital'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.index[data['Salida_Capital'] == 1], \n",
    "    y=data['Close'][data['Salida_Capital'] == 1], \n",
    "    mode='markers', \n",
    "    marker=dict(size=10, color='blue', symbol='triangle-down'), \n",
    "    name='Salida de Capital'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.index[data['Se√±al_Compra'] == 1], \n",
    "    y=data['Close'][data['Se√±al_Compra'] == 1], \n",
    "    mode='markers', \n",
    "    marker=dict(size=10, color='yellow', symbol='triangle-up'), \n",
    "    name='Se√±al de Compra'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=f'Se√±ales Institucionales para {ticker}',\n",
    "    xaxis_title='Fecha',\n",
    "    yaxis_title='Precio',\n",
    "    xaxis_rangeslider_visible=False,\n",
    "    template='plotly_dark',\n",
    "    height=600,\n",
    "    width=1000\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Verificar valores de 'Valoraci√≥n'\n",
    "print(\"Valores de 'Valoraci√≥n':\")\n",
    "print(data[['Valoraci√≥n']].dropna().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Obtenci√≥n de datos fundamentales desde Yahoo Finance\n",
    "def obtener_valuacion_yahoo(ticker):\n",
    "    try:\n",
    "        print(f\"üìä Descargando datos fundamentales para {ticker} desde Yahoo Finance...\")\n",
    "        stock = yf.Ticker(ticker)\n",
    "        info = stock.info\n",
    "        pe_ratio = info.get(\"trailingPE\", None)\n",
    "        pb_ratio = info.get(\"priceToBook\", None)\n",
    "        ev_to_ebitda = info.get(\"enterpriseToEbitda\", None)\n",
    "        valid_metrics = [m for m in [pe_ratio, pb_ratio, ev_to_ebitda] if m is not None]\n",
    "        valuation_index = sum(valid_metrics) / len(valid_metrics) if valid_metrics else None\n",
    "        return pe_ratio, pb_ratio, ev_to_ebitda, valuation_index\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error obteniendo datos fundamentales: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "pe_ratio, pb_ratio, ev_to_ebitda, valuation_index_yahoo = obtener_valuacion_yahoo(ticker)\n",
    "if valuation_index_yahoo is not None:\n",
    "    print(f\"üìä P/E Ratio: {pe_ratio if pe_ratio else 'N/A'} | P/B Ratio: {pb_ratio if pb_ratio else 'N/A'} | EV/EBITDA: {ev_to_ebitda if ev_to_ebitda else 'N/A'}\")\n",
    "    print(f\"üìä √çndice de Valoraci√≥n (Yahoo): {valuation_index_yahoo:.2f}\")\n",
    "    if valuation_index_yahoo > 2.5:\n",
    "        print(\"‚ö†Ô∏è La empresa est√° MUY SOBREVALORADA üìà (Riesgo de ca√≠da)\")\n",
    "    elif valuation_index_yahoo < 1:\n",
    "        print(\"‚úÖ La empresa est√° INFRAVALORADA üìâ (Oportunidad de compra)\")\n",
    "    else:\n",
    "        print(\"üîç La empresa tiene una valoraci√≥n razonable.\")\n",
    "else:\n",
    "    print(\"‚ùå No se pudo calcular el √≠ndice de valoraci√≥n desde Yahoo Finance.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Agregar 'Valoraci√≥n' como caracter√≠stica y actualizar el conjunto de entrenamiento\n",
    "features = ['RSI', 'MACD', 'Signal_Line', '%K', '%D', 'Volume', 'Institutional_Index', 'Valoraci√≥n']\n",
    "X = data[features].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## üìå Gr√°fico del √çndice de Valoraci√≥n con suavizado\n",
    "plt.figure(figsize=(12, 5))\n",
    "data['Valoraci√≥n_Suave'] = data['Valoraci√≥n'].rolling(window=5).mean()\n",
    "plt.plot(data.index, data['Valoraci√≥n_Suave'], color='blue', linewidth=2, label=\"√çndice de Valoraci√≥n (Suavizado)\")\n",
    "min_valor = data['Valoraci√≥n'].min()\n",
    "max_valor = data['Valoraci√≥n'].max()\n",
    "margen = (max_valor - min_valor) * 0.1\n",
    "plt.ylim(min_valor - margen, max_valor + margen)\n",
    "plt.axhline(2.5, linestyle='--', color='red', alpha=0.5, label=\"Sobrevalorada (>2.5)\")\n",
    "plt.axhline(1, linestyle='--', color='green', alpha=0.5, label=\"Infravalorada (<1)\")\n",
    "plt.title(f'√çndice de Valoraci√≥n de {ticker}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NOMBRE_ENTORNO)",
   "language": "python",
   "name": "nombre_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
